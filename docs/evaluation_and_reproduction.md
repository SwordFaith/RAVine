# How to run evaluation?


We provide two ways of evaluation. One is the main way, which is to run and evaluate the model completely. The other is to evaluate based on the running log, which does not involve model running and LLM-as-Judge calls, so it can be used to reproduce experimental results.

Btw, everyone is welcome to submit and evaluate their own models!

## Evaluation

*Please follow the installation guide in [README](https://github.com/SwordFaith/RAVine?tab=readme-ov-file#how-to-runevaluate) to complete the environment construction before evaluation.*


First, data preparation. We recommend that you localize the necessary data first. You can run `python download.py` directly in the root directory. Then the required data will be placed in the `\data` directory.


Second, you need to write one or more configuration files corresponding to the models and settings you want to evaluate. Take a configuration file template `llama3_32k_bm25.yaml` provided in the `\configs` directory as an example:

```yaml
agent_model_name: /path/to/your/model/
tensor_parallel_size: 8
dtype: auto
api_key: token-abc123
tool_call_parser: llama3_json
max_model_len: 32768
nuggets_path: /path/to/nuggets/file/
qrels_path: /path/to/qrels/file/
corpus_name: msmarco_v2.1
index_path: /path/to/bm25/index/
mapper_path: /path/to/url2doc/mapper/file/
search_client_type: bm25
eval_model: gemini-2.5-flash-preview-05-20-nothinking
log_dir: /path/to/log/
enable_reasoning: false
rope_scaling: false
chat_template: configs/chat/tool_chat_template_llama3.1_json.jinja
latex_table: /path/to/result/txt/
```
What you may need to modify:
- `agent_model_name`: choose a model as an agent, which participates in iterative search and final report generation. Write the model's path in hf repo or in local.
- `tensor_parallel_size`: number of GPUs per node for vllm service.
- `tool_call_parser`: tool call parser in vllm. `hermes` is recommended for qwen series models and `llama3_json` is recommended for llama3 series models.
- `max_model_len`: Maximum context length. In order to support the completion of most tasks, please try to set it to 32k or more.
- `nuggets_path`: jsonline file path for nuggets.
- `qrels_path`: jsonline file path for qrels.
- `index_path`: path to the corpus index file you want to use.
- `mapper_path`: file path of the mapper for urls and web pages.
- `search_client_type`: the index type. Currently only `bm25` and `dense` are supported.
- `eval_model`: the LLM-as-Judge name, used for nugget mining, merging, and scoring during nuggetization, and nuggets assignment during evaluation. After practice, we recommend using `gemini-2.5-flash`, which has good effect and low cost.
- `log_dir`: log directory, which is to store the running logs, the results of each data point, and the results of the entire test set.
- `chat_template`: The model's chat template which should support tool calling. If not set, the default template is used.
- `latex_table`: If this file path is assigned, the test set results will be automatically appended to the file in the format of latex tabular lines.
- `enable_reasoning`: whether to support thinking. This is useful for some models that support thinking switching (such as Qwen3).
- `rope_scaling`: some models require this additional parameter when increasing the context length. If necessary, please check carefully which rope scaling the model you are evaluating uses.


Third, add your configuration file to the run command, or to a script when you need to run multiple configuration files at once.

For example, to run a single configuration such as `llama3_32k_bm25.yaml`,
```bash
source vllm_env
bash scripts/server/vllm.sh configs/llama3_32k_bm25.yaml
source src_env
bash scripts/evaluation/run.sh configs/llama3_32k_bm25.yaml
```
To run multiple configurations, add the configuration file name to the `CONFIGS` variable in `\scripts\evaluation\run_all_eval.sh`, and run
```bash
bash scripts/evaluation/run_all_eval.sh
```

When the run is completed, the test set results will be output and saved in `final_eval.json` under the `log_dir` directory you set, and the test results of each data point will be saved in `eval.jsonl`. In addition, you can also find the context history of each data point under `log_dir`.


## Reproduction

During the reproduction phase, our requirements for data and configuration files are the same as those for the initial evaluation. Considering the instability of LLM-as-Judge and agentic LLMs, our reproduction will be based on the running logs stored during the initial evaluation. These logs save all the content generated by the model or returned by the search tools in each round and the nugget evaluation records of the final report by LLM-as-Judge, which make our results reproducible.


Similarly, to reproduce a single configuration file such as `llama3_32k_bm25.yaml`,
```bash
source vllm_env
bash scripts/server/vllm.sh configs/llama3_32k_bm25.yaml
source src_env
bash scripts/evaluation/run_log.sh configs/llama3_32k_bm25.yaml
```
To reproduce multiple configurations, add the configuration file name to the `CONFIGS` variable in `\scripts\evaluation\run_all_eval_log.sh`, and run
```bash
bash scripts/evaluation/run_all_eval_log.sh
```

